{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a Jupyter notebook cell\n",
    "!pip install pandas numpy seaborn tensorflow scikit-learn keras joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense , Bidirectional, BatchNormalization, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import joblib\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('phishing_site_urls_updated.csv')\n",
    "    print(df)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to read the CSV file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Cleaning: Remove both http(s) and www, and trailing slashes\n",
    "df['URL'] = (\n",
    "    df['URL']\n",
    "    .str.replace(r'^https?://', '', regex=True)\n",
    "    .str.replace(r'^www\\.', '', regex=True)\n",
    "    .str.rstrip('/')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_lengths = df['URL'].apply(len)\n",
    "print(url_lengths.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Use stratified split for balanced classes\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['URL'], df['Label'], test_size=0.2, stratify=df['Label'], random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Training labels size: {y_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Test labels size: {y_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (Traditional NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3))  # Use character-level n-grams\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character Embeddings (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Remove duplicate Tokenizer fitting (keep only one)\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building\n",
    "Model 1: Logistic Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2: LSTM (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Set max_len dynamically based on URL length distribution\n",
    "max_len = min(100, int(np.percentile(df['URL'].apply(len), 95)))\n",
    "\n",
    "# Pad sequences to equal length\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_train_pad = np.array(X_train_pad)\n",
    "X_test_pad = np.array(X_test_pad)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Calculate class weights for imbalanced data\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "# Enhanced LSTM Model Architecture\n",
    "model_lstm = Sequential([\n",
    "    # Increased embedding dimension with masking\n",
    "    Embedding(input_dim=len(tokenizer.word_index)+1, \n",
    "                output_dim=256,\n",
    "                input_length=max_len,\n",
    "                mask_zero=True),\n",
    "    \n",
    "    # First Bidirectional LSTM with regularization\n",
    "    Bidirectional(LSTM(128, return_sequences=True,\n",
    "                      kernel_regularizer=l2(0.01))),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Second Bidirectional LSTM\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dropout(0.3),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Custom optimizer with learning rate decay\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "model_lstm.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    min_delta=0.001\n",
    ")\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=3,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Train with class weights and callbacks\n",
    "history = model_lstm.fit(\n",
    "    X_train_pad,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stopping, lr_scheduler],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For logistic regression\n",
    "y_pred = model_lr.predict(X_test_tfidf)\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred))\n",
    "\n",
    "# 6. Add ROC-AUC for both models in evaluation\n",
    "y_pred_proba = model_lr.predict_proba(X_test_tfidf)[:, 1]\n",
    "print(\"Logistic Regression ROC-AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "\n",
    "# For LSTM\n",
    "y_pred_lstm = (model_lstm.predict(X_test_pad) > 0.5).astype(int)\n",
    "print(\"LSTM F1-Score:\", f1_score(y_test, y_pred_lstm))\n",
    "\n",
    "y_pred_lstm_proba = model_lstm.predict(X_test_pad)\n",
    "print(\"LSTM ROC-AUC:\", roc_auc_score(y_test, y_pred_lstm_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False Positive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = X_test[(y_pred == 1) & (y_test == 0)]\n",
    "print(\"False Positives:\", false_positives.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Save models for deployment\n",
    "model_lr_path = \"logistic_regression_phishing.pkl\"\n",
    "joblib.dump(model_lr, model_lr_path)\n",
    "model_lstm.save(\"lstm_phishing_model.h5\")\n",
    "\n",
    "# 8. Improve prediction functions with HTTPS and trailing slash cleaning\n",
    "def clean_url(url):\n",
    "    return (\n",
    "        url.replace('http://', '')\n",
    "           .replace('https://', '')\n",
    "           .replace('www.', '')\n",
    "           .rstrip('/')\n",
    "    )\n",
    "\n",
    "def predict_phishing(url):\n",
    "    url_clean = clean_url(url)\n",
    "    url_tfidf = vectorizer.transform([url_clean])\n",
    "    return \"Phishing\" if model_lr.predict(url_tfidf)[0] == 1 else \"Legitimate\"\n",
    "\n",
    "# Test\n",
    "# print(predict_phishing(\"google.com\"))          # Legitimate\n",
    "print(predict_phishing(\"www.avedeoiro.com/site/plugins/chase/\"))   # Phishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_phishing_lstm(url):\n",
    "    url_clean = clean_url(url)\n",
    "    url_seq = tokenizer.texts_to_sequences([url_clean])\n",
    "    url_padded = pad_sequences(url_seq, maxlen=max_len)\n",
    "    prediction_prob = model_lstm.predict(url_padded)[0][0]\n",
    "    print(f\"Probability: {prediction_prob:.3f}\")\n",
    "    return \"Phishing\" if prediction_prob > 0.5 else \"Legitimate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Legitimate URL\n",
    "print(predict_phishing_lstm(\"google.com\"))          # Output: Legitimate\n",
    "\n",
    "# Example 2: Phishing URL\n",
    "print(predict_phishing_lstm(\"paypal-login.scam\"))   # Output: Phishing\n",
    "\n",
    "# Example 3: Edge Case\n",
    "print(predict_phishing_lstm(\"www.agenciasports.com/wp-includes/images/smilies/mbafresh.htm\"))  # Depends on training data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
